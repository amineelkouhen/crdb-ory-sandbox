= Ory + CockroachDB Joint PoC Environment
:linkattrs:
:project-owner: amineelkouhen
:project-name:  crdb-ory-sandbox
:project-group: com.cockroachlabs
:project-version:   1.0.0
:site-url:  https://github.com/amineelkouhen/crdb-ory-sandbox

image:https://img.shields.io/github/contributors/{project-owner}/{project-name}[GitHub contributors]
image:https://img.shields.io/github/forks/{project-owner}/{project-name}[Fork]
image:https://img.shields.io/github/stars/{project-owner}/{project-name}[GitHub Repo stars]
image:https://img.shields.io/github/watchers/{project-owner}/{project-name}[GitHub watchers]
image:https://img.shields.io/github/issues/{project-owner}/{project-name}[GitHub issues]
image:https://img.shields.io/github/license/{project-owner}/{project-name}[License]

image::images/banner.png[banner]
---

This project provides a comprehensive set of *Terraform* templates to provision a joint *Ory* and *CockroachDB* proof-of-concept (PoC) environment on multiple cloud providers. The goal is to demonstrate a resilient, distributed IAM architecture that combines Ory's identity stack with CockroachDB's globally consistent SQL database.

== üìã Prerequisites

- Install https://learn.hashicorp.com/tutorials/terraform/install-cli[Terraform^]
- Ensure SSH access by creating a key pair (e.g., ~/.ssh/id_rsa)
- Obtain an https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html[AWS Access Key and Secret^]
- Update `variables.tf` with your AWS credentials (aws_access_key and aws_secret_key)
- For CockroachDB: obtain a valid CockroachDB License and place the key in the `cluster_license` variable (variables.tf)
- For Ory Enterprise: obtain a valid Ory Enterprise License (OEL) and place the credentials file as `credentials.json` in the `resources` directory

== ü™≥ CockroachDB - Architecture

CockroachDB is a distributed SQL database designed for horizontal scalability, high availability, and strong consistency. It stores data using a transactional key-value layer, automatically splitting and replicating ranges as data grows. This ensures resilience across failures‚Äîeven full AZ or region-level outages‚Äîwithout manual intervention.

CockroachDB scales horizontally with minimal operator overhead: At the key-value level, CockroachDB starts off with a single, empty range. As you put data in, this single range eventually reaches a threshold size. When that happens, the data splits into two ranges, each covering a contiguous segment of the entire key-value space. This process continues indefinitely; as new data flows in, existing ranges continue to split into new ranges, aiming to keep a relatively small and consistent range size.

image::images/ranges.gif[CRL]

A CockroachDB cluster consists of identical nodes deployed across multiple availability zones (AZs), offering ACID guarantees and global SQL access.

image::images/multi-regional.png[CRL]

== üëÆ‚Äç Ory - Architecture

Ory provides modular, cloud-native IAM tools built for distributed systems. Key components include:

- https://www.ory.sh/kratos[Ory Kratos^] for identity management (including users, groups, and organizations),
- https://www.ory.sh/hydra[Ory Hydra^] for OAuth2 and OIDC flows,
- https://www.ory.sh/keto[Ory Keto^] for fine-grained authorization and relationship-based access control (ReBAC,  inspired by Google Zanzibar)
- https://www.ory.sh/oathkeeper[Ory Oathkeeper^] for edge authentication and request-level access control with an Identity and Access Proxy.

Each service is stateless and communicates with CockroachDB as its persistent data store.

image::images/fig1_Ory_Architecture_Overview.png[Ory]

== ü§ù Ory/CockroachDB joint architecture

The PoC environment integrates:

- Ory Kratos handled identity management, including user credentials, profile metadata, and sessions.
- Ory Hydra served as the OAuth2 and OIDC provider, managing token issuance, consent flows, and client registration.
- Ory Keto implemented fine-grained access control based on Google Zanzibar-inspired relationships.

Each of these components relied on CockroachDB to store their state in a consistent and durable way, enabling them to function correctly even in the presence of partial outages or regional network partitions.

Ory‚Äôs architecture is well-suited to operate with CockroachDB because of its stateless design and API-first philosophy. Each component can be deployed as a stateless service, with its only persistence requirement being the backing SQL database. This makes it straightforward to horizontally scale services, perform rolling updates, or deploy new regions without having to orchestrate complex data migrations.

CockroachDB, in turn, provides the always-consistent database layer that ensures user identities, access control rules, and session tokens are always accurate ‚Äî no matter which region is serving a request.

image::images/fig4_Ory_CockroachDB_multi_region_deployment_architecture.png[Ory-CRDB]

From a technical standpoint, identity and authorization in this system are modeled as structured entities within CockroachDB. The use of CockroachDB‚Äôs strongly consistent transactions ensures that IAM operations remain correct, even when issued concurrently from different regions or under network duress.

== üì¶ Deployment Options

Deployment templates are organized under the `main/` directory with configurations for various cloud providers:

[cols="25h,~,~"]
|===
^.^h|Cloud Provider
^.^h|Single-Region Deployment ^.^h|Multi-Region Deployment

^.^|Amazon Web Services (AWS)

a|image::main/aws/Single-Region-Multi-AZ/images/Single-Region-MAZ.svg[Basic, link="main/aws/Single-Region-Multi-AZ"]
a|image::images/coming_soon.png[Multi-Zonal, link="main/aws/Multi-Region-Multi-AZ"]

^.^|Google Cloud Platform (GCP)

a|image::images/coming_soon.png[Basic Cluster, link=""]
a|image::images/coming_soon.png[Basic Cluster, link=""]

^.^|Microsoft Azure

a|image::images/coming_soon.png[Basic Cluster, link=""]
a|image::images/coming_soon.png[Basic Cluster, link=""]

|===

=== üöÄ Usage Guide

Each configuration uses .tf.json files that reference Terraform modules for:

- Networking (VPCs, subnets, load balancers)
- DNS and FQDN setup
- CockroachDB clusters
- EKS (Ory) clusters
- Bastion host setup

See examples in `main/aws/Single-Region-Multi-AZ/` for AWS deployments.

Example of a configuration file

[source,json]
----
{
    "provider": {
        "aws": {
            "region": "${var.region_name}",
            "access_key": "${var.aws_access_key}",
            "secret_key": "${var.aws_secret_key}"
        }
    },
    "module": {
        "network-vpc": {
            "source": "../../../../modules/aws/network",
            "name": "${var.deployment_name}-${var.env}",
            "vpc_cidr": "${var.vpc_cidr}",
            "subnets_cidrs": "${var.subnets}",
            "resource_tags" : {}
        },
        "keypair": {
            "source": "../../../../modules/aws/keypair",
            "name": "${var.deployment_name}-${var.env}",
            "ssh_public_key": "${var.ssh_public_key}",
            "resource_tags" : {}
        },
        "crdb-cluster": {
            "source": "../../../modules/aws/cr",
            "name": "${var.deployment_name}-${var.env}",
            "worker_count": "${var.crdb_cluster_size[0]}",
            "machine_type": "${var.crdb_machine_type}",
            "machine_image": "${var.crdb_machine_images[0]}",
            "ssh_user": "${var.ssh_user}",
            "ssh_public_key": "${var.ssh_public_key}",
            "ssh_key_name": "${module.keypair.key-name}",
            "security_groups": "${module.network-vpc.security-groups}",
            "region": "${var.regions[0]}",
            "availability_zones": "${keys(var.crdb_subnets[0])}",
            "subnets": "${module.network-vpc.subnets}",
            "cockroach_release": "${var.crdb_release}",
            "boot_disk_size" : "${var.volume_size}",
            "boot_disk_type" : "${var.volume_type}",
            "resource_tags": {}
        },
        "ory-cluster": {
            "source": "../../../modules/aws/eks",
            "deployment_name": "${var.deployment_name}-${var.env}",
            "cluster_version": "${var.eks_version}",
            "vpc_cidr": "${var.eks_vpc_cidr}",
            "subnets_cidrs": "${var.eks_public_subnets[0]}",
            "machine_type": "${var.eks_machine_type}",
            "machine_image": "${var.eks_machine_image}",
            "cluster_size": "${var.eks_cluster_size}",
            "disk_size": "${var.eks_volume_size}",
            "resource_tags": {}
        }
    }
}
----

A standalone EC2 client (bastion) is created with all the components and required CLIs/Tools to start working on the joint environment.

To perform tests - After provisioning:

1- SSH into the bastion host using the public IP from Terraform outputs.
----
Outputs:
####################################### Client #######################################

client-public-IP = "a.b.c.d"
----

----
$ ssh -i ~/.ssh/id_rsa ubuntu@a.b.c.d
----
2- Check setup status via `/home/ubuntu/prepare_client.log`. Wait till you have the following massage:
----
Thu Jun 12 16:52:36 UTC 2025 - üíØ Client setting Completed
----

You can also check that all Ory services are up by executing `kubectl get svc`
----
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)          AGE
ory-keto-read          LoadBalancer   172.20.196.253   a787162f2fc80462d828f10ecac85b2c-359681629.us-east-1.elb.amazonaws.com    4466:30486/TCP   6m13s
ory-keto-write         LoadBalancer   172.20.253.187   a7200f25b0532411abf8c6ea38c1d388-371428682.us-east-1.elb.amazonaws.com    4467:31930/TCP   6m13s
ory-oel-hydra-admin    LoadBalancer   172.20.195.225   a2189696182c94455b020230b1721dce-691944772.us-east-1.elb.amazonaws.com    4445:30850/TCP   7m59s
ory-oel-hydra-public   LoadBalancer   172.20.26.176    a5b587d0c2b1a477a8aafbaa7a458c4a-1033217843.us-east-1.elb.amazonaws.com   4444:31742/TCP   7m59s
----

Congratulations üéâ Now, you can start testing Ory capabilities. All environment variables required for Ory components are pre-configured for testing.

=== üß™ Testing Ory Components

==== ‚úÖ Hydra (OAuth 2.0 Provider)

Ory Hydra is a server implementation of the OAuth 2.0 authorization framework and the OpenID Connect Core 1.0. It tracks clients, consent requests, and tokens with strong consistency to prevent replay attacks or duplicate authorizations.

The OAuth 2.0 authorization framework enables a third-party application to obtain limited access to an HTTP service, either on behalf of a resource owner by orchestrating an approval interaction between the resource owner and the HTTP service, or by allowing the third-party application to obtain access on its own behalf.

image::images/fig7_OAuth2_Flow.png[Hydra-flow]

The OAuth 2.0 authorization flow involving a client application, the resource owner, Ory Hydra (as the authorization server), and the resource server is structured as follows:

image::images/fig8_interaction_flow_using_Ory_Hydra.png[Hydra]

The sequence diagram depicts the interactions between four key components:

- the Client
- the Resource Owner (typically the user)
- Ory Hydra
- the Resource Server (the API or service that hosts protected resources).

The flow begins when the Client ‚Äî an application seeking access to protected resources ‚Äî initiates a request for authorization from the Resource Owner. This typically takes the form of a redirect to a login or consent screen provided by the Authorization Server (Ory Hydra). The Resource Owner reviews the request and, upon granting access, provides an authorization grant (often an authorization code) to the client.

Next, the Client uses this authorization grant to request an access token from Ory Hydra. Along with the grant, the client also authenticates itself (using credentials such as a client ID and secret). Ory Hydra validates the authorization grant and client credentials. If everything checks out, it responds by issuing an access token to the client.

Armed with the access token, the Client then makes a request to the Resource Server, presenting the token as proof of authorization. The Resource Server validates the access token ‚Äî often by introspecting it via Hydra or verifying its signature if it‚Äôs a JWT (JSON Web Token) ‚Äî and, if valid, serves the requested protected resource to the client.

This flow encapsulates the standard Authorization Code Grant pattern in OAuth 2.0, with Ory Hydra fulfilling the role of a secure, standards-compliant authorization server that manages token issuance, validation, and policy enforcement. It's designed to separate concerns between applications and services, enabling scalable and secure delegated access.

To test Ory Hydra, you can create an OAuth2 client, generate an access token, then introspect it using the following Hydra commands:

[source,bash]
----
$ hydra create oauth2-client --endpoint $HYDRA_ADMIN_URL --format json --grant-type client_credentials
----

[source,json]
----
{
    "client_id": "9692d3f9-fcdc-4526-80c4-fc667d959a5f",
    "client_name": "",
    "client_secret": "F-~KQ8bKSeTxBKdZSS6woHSs9C",
    "client_secret_expires_at": 0,
    "client_uri": "",
    "created_at": "2025-06-11T16:43:07Z",
    "grant_types": ["client_credentials"],
    "jwks": {},
    "logo_uri": "",
    "metadata": {},
    "owner": "",
    "policy_uri": "",
    "registration_access_token": "ory_at_8xQlVk7rA_MX1yenToVmA7Wr7MLOLXJZdhh9iYHDEAQ.xGPfP4-AiGuOxAKkX-ZIdSntOJo8fy3a4b75ckE_V-g",
    "registration_client_uri": "http://public.hydra.localhost:4444/oauth2/register/",
    "request_object_signing_alg": "RS256",
    "response_types": ["code"],
    "scope": "offline_access offline openid",
    "skip_consent": false,
    "skip_logout_consent": false,
    "subject_type": "public",
    "token_endpoint_auth_method": "client_secret_basic",
    "tos_uri": "",
    "updated_at": "2025-06-11T16:43:07.320505Z",
    "userinfo_signed_response_alg": "none"
}
----

[source,bash]
----
$ hydra perform client-credentials --endpoint $HYDRA_PUBLIC_URL --client-id 9692d3f9-fcdc-4526-80c4-fc667d959a5f --client-secret F-~KQ8bKSeTxBKdZSS6woHSs9C
----

[source,bash]
----
ACCESS TOKEN	ory_at_A2TpIR394rnUOtA0PLhvARKQyODmLIH7Fer5Y8clwe8.J61E8kR3ZH2w529D-5HOkuqoaTZy-CNLlNtvunYpdjg
REFRESH TOKEN	<empty>
ID TOKEN	<empty>
EXPIRY		2025-06-11 19:49:39 +0200 CEST
----

[source,bash]
----
$ hydra introspect token --format json-pretty --endpoint $HYDRA_ADMIN_URL ory_at_A2TpIR394rnUOtA0PLhvARKQyODmLIH7Fer5Y8clwe8.J61E8kR3ZH2w529D-5HOkuqoaTZy-CNLlNtvunYpdjg
----

[source,json]
----
{
    "active": true,
    "client_id": "9692d3f9-fcdc-4526-80c4-fc667d959a5f",
    "exp": 1749664180,
    "iat": 1749660580,
    "iss": "http://public.hydra.localhost:4444",
    "nbf": 1749660580,
    "sub": "9692d3f9-fcdc-4526-80c4-fc667d959a5f",
    "token_type": "Bearer",
    "token_use": "access_token"
}
----

==== ‚úÖ Kratos

Coming Soon

==== ‚úÖ Keto (Authorization Engine)

Keto provides scalable access control as relationships (ReBAC-style authorization).

In Ory Keto, authorization is checked by evaluating whether a relation tuple exists (directly or through recursive expansion) that permits a given subject to perform a relation on an object in a namespace. This data model is designed for high scalability and flexibility, enabling complex access patterns like group membership, role inheritance, and hierarchical access rights.

A permission model is a set of rules that define which relations are checked in the database during a permission check.

Permission checks are answered based on:

The data available in CockroachDB, for example: `User:Bob is owner of Document:X`

Permission rules, for example: `All owners of a document can view it`.
When you ask Keto Permissions: is `User:Bob allowed to view on Document:X`, the system checks up how Bob could have the view permission, and then checks if Bob is owner of the `document X`. The permission model tells Ory Permissions what to check in the database.

image::images/permission_graph.png[Keto]

To test Ory Keto, you can create a relation tuple using the Keto SDK:

[source,bash]
----
$ echo '{"namespace":"documents","object":"doc-123","relation":"viewer","subject_id":"user:alice"}'  | keto relation-tuple create /dev/stdin --insecure-disable-transport-security
----

or by using the Keto REST API:

[source,bash]
----
$ curl -i -X PUT "$KETO_WRITE_REMOTE"/admin/relation-tuples \
-H "Content-Type: application/json" \
-d '{"namespace":"documents","object":"doc-123","relation":"viewer","subject_id":"user:alice"}'
----

You can use Ory Keto's expand-API to display who has access to an object, and why:

[source,bash]
----
$ keto expand viewer documents photos --insecure-disable-transport-security
----

To assist users with managing permissions for their files, the application has to display who has access to a file and why. In this example, we assume that the application knows the following files and directories:

[source,bash]
----
‚îú‚îÄ photos            (owner: maureen; shared with laura)
   ‚îú‚îÄ beach.jpg      (owner: maureen)
   ‚îú‚îÄ mountains.jpg  (owner: laura)
----

It's important to test your permission model. To test the permissions manually, you can create relationships and check permissions through the API or SDK.

[source,bash]
----
$ keto check \"user:alice\" viewer documents /photos/beach.jpg --insecure-disable-transport-security
> allowed
----

=== ‚öôÔ∏è Terraform Execution

To initialize and deploy:

----
terraform init
terraform plan
terraform apply
----

Terraform will provision two logical clusters with:

- For CockroachDB:

    * VPC and subnets (each in a distinct Availability Zone)
    * Network Load Balancers
    * 3-node CockroachDB cluster (each worker in a distinct subnet)

- For Ory:
    * VPC and subnets (each in a distinct Availability Zone)
    * 3-worker EKS cluster (each worker in a distinct subnet)
    * Ory pods are exposed as services behind Load Balancers

- A Bastion host with all requires SDKs and tools pre-installed

Sample output includes URLs and IPs for the deployed environment:

....
Outputs:
####################################### Client #######################################

client-public-IP = "52.40.254.77"

####################################### CRDB Cluster #################################

console-url = "http://amine.cluster.sko-iam-demo.com:8080/"
connexion-string = "postgresql://root@amine.cluster.sko-iam-demo.com:26257/defaultdb"
console-url = "http://nlb-2025061115420336750000000b-6069ff0a543288f4.elb.us-east-1.amazonaws.com:8080/"
crdb-cluster-nlb-dns = "nlb-2025061115420336750000000b-6069ff0a543288f4.elb.us-east-1.amazonaws.com"

crdb-cluster-private-ips = [
  "10.1.1.75",
  "10.1.2.176",
  "10.1.3.188",
]
crdb-cluster-public-ips = [
  "174.129.63.86",
  "54.226.135.115",
  "54.242.175.190",
]

####################################### EKS Ory Cluster #################################

ory-cluster-endpoint = "https://3545FA6F561797A27DFD79508CA391D4.gr7.us-east-1.eks.amazonaws.com"
ory-cluster-name = "amine-us-ory-cluster"
....

=== üóÇ To-Do List (In Progress)

- Add demo tooling and simulators for Kratos and Hydra (the https://github.com/viragtripathi/crdb-ory-keto-demo[Keto load simulator^] is already done by Chatur üë≤üèæ)
- Automate certification test suite
- Extend support to GCP and Azure
